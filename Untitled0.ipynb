{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3mRzFheNTimM4z21LiLo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "342709a4562f40ac8d37eb04d19018b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Images dir:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2685d8bbbc924ef88d8d15bae5605f89",
            "placeholder": "​",
            "style": "IPY_MODEL_51e16b850e0a47618b3bb31b29893682",
            "value": "images/demo"
          }
        },
        "2685d8bbbc924ef88d8d15bae5605f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e16b850e0a47618b3bb31b29893682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c44be477bb547f8abf0589b5e35dcc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "one image",
              "multiple images"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Task:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_e99cad31f98148fdb7b9e530bc1fbaa3",
            "style": "IPY_MODEL_50ad7dfded034546bf3b83d9d11b346a"
          }
        },
        "e99cad31f98148fdb7b9e530bc1fbaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ad7dfded034546bf3b83d9d11b346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjd9021/ImageSearch/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0MycrEu9XeUV"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output, display, Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/xinyu1205/recognize-anything.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gyVJ1NZZHwD",
        "outputId": "4e94705c-275d-4103-fe30-3fbe54579f60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recognize-anything'...\n",
            "remote: Enumerating objects: 527, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 527 (delta 212), reused 204 (delta 168), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (527/527), 23.25 MiB | 28.58 MiB/s, done.\n",
            "Resolving deltas: 100% (278/278), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd recognize-anything"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfPdZDtSjT_Z",
        "outputId": "9b8887a7-c5fa-4b4b-d2cd-da4de108cf85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/recognize-anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm transformers fairscale pycocoevalcap"
      ],
      "metadata": {
        "id": "r5dF_5o4ZQwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05976ca4-9d27-4c92-f8ae-6fdb99c9312b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap) (2.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332108 sha256=ec694b82700f9045636e26a68ef6187a19fda8419f68d78ebc6f98285f0e695d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, pycocoevalcap, timm, fairscale\n",
            "Successfully installed fairscale-0.4.13 huggingface-hub-0.16.4 pycocoevalcap-1.2 safetensors-0.3.2 timm-0.9.5 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'RAM'\n",
        "def download_checkpoints(model):\n",
        "    print('You selected', model)\n",
        "    if not os.path.exists('pretrained'):\n",
        "        os.makedirs('pretrained')\n",
        "\n",
        "    if model == \"RAM\":\n",
        "        ram_weights_path = 'pretrained/ram_swin_large_14m.pth'\n",
        "        if not os.path.exists(ram_weights_path):\n",
        "            !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/ram_swin_large_14m.pth -O pretrained/ram_swin_large_14m.pth\n",
        "        else:\n",
        "            print(\"RAM weights already downloaded!\")\n",
        "    else:\n",
        "        tag2text_weights_path = 'pretrained/tag2text_swin_14m.pth'\n",
        "        if not os.path.exists(tag2text_weights_path):\n",
        "            !wget https://huggingface.co/spaces/xinyu1205/Recognize_Anything-Tag2Text/resolve/main/tag2text_swin_14m.pth -O pretrained/tag2text_swin_14m.pth\n",
        "        else:\n",
        "            print(\"Tag2Text weights already downloaded!\")\n",
        "\n",
        "download_checkpoints(model)\n",
        "print(model, 'weights are downloaded!')\n"
      ],
      "metadata": {
        "id": "uDvXKUU8ZjLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir_widget = widgets.Text(value=\"images/demo\", description=\"Images dir:\")\n",
        "display(images_dir_widget)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "342709a4562f40ac8d37eb04d19018b3",
            "2685d8bbbc924ef88d8d15bae5605f89",
            "51e16b850e0a47618b3bb31b29893682"
          ]
        },
        "id": "wxmogBqobpys",
        "outputId": "483d0a5a-fac7-4791-ba1a-f877202421d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='images/demo', description='Images dir:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "342709a4562f40ac8d37eb04d19018b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = images_dir_widget.value\n",
        "print(images_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wp7Iwkmb7BX",
        "outputId": "0aa41ea4-9eae-449a-fc66-8ec8609af52a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images/demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_files = [f\"{images_dir}/{file}\" for file in sorted(os.listdir(images_dir)) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "image_path = image_files[0]\n",
        "\n",
        "# image_path = '/MicrosoftTeams-image (1).png'\n",
        "# Create dropdown widget\n",
        "image_dropdown = widgets.Dropdown(\n",
        "    options=image_files,\n",
        "    description='Select Image:',\n",
        ")\n",
        "\n",
        "# Create image preview widget\n",
        "image_preview = widgets.Output()\n",
        "\n",
        "# Define function to update image preview\n",
        "def update_preview(change):\n",
        "    global image_path\n",
        "    image_path = change.new\n",
        "    with image_preview:\n",
        "        image_preview.clear_output()\n",
        "        display(Image(filename=image_path, width=400))\n",
        "\n",
        "# Set the initial image preview\n",
        "with image_preview:\n",
        "    display(Image(filename=image_files[0], width=400))\n",
        "\n",
        "# Attach the update function to the dropdown\n",
        "image_dropdown.observe(update_preview, names='value')\n",
        "\n",
        "# Display the widgets\n",
        "display(image_dropdown, image_preview)"
      ],
      "metadata": {
        "id": "2k6hcyZEb-Pd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_widget = widgets.Dropdown(\n",
        "    options=[\"one image\", \"multiple images\"],\n",
        "    value=\"one image\",\n",
        "    description=\"Task:\"\n",
        ")\n",
        "display(task_widget)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9c44be477bb547f8abf0589b5e35dcc9",
            "e99cad31f98148fdb7b9e530bc1fbaa3",
            "50ad7dfded034546bf3b83d9d11b346a"
          ]
        },
        "id": "V-sNTJJOdrrQ",
        "outputId": "9b44e6ae-4990-47b1-a010-4b1e5f018318"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Task:', options=('one image', 'multiple images'), value='one image')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c44be477bb547f8abf0589b5e35dcc9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task = task_widget.value\n",
        "print(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkkcIbhSdwA1",
        "outputId": "33a85e0a-e11b-4802-9e1e-3fb0d463b718"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images/demo/demo1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s7ldgIDvmM1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# image_path = \"path_to_your_image.jpg\"\n",
        "pretrained_path = \"pretrained/ram_swin_large_14m.pth\"\n",
        "\n",
        "# Construct the command\n",
        "command = [\n",
        "    \"python\",\n",
        "    \"inference_ram.py\",\n",
        "    \"--image\",\n",
        "    image_path,\n",
        "    \"--pretrained\",\n",
        "    pretrained_path\n",
        "]\n",
        "\n",
        "# Run the command and capture the output\n",
        "completed_process = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "# Get the captured output\n",
        "output_text = completed_process.stdout\n",
        "\n",
        "# Store or process the output_text as needed\n",
        "# print(\"Output from inference_ram.py:\")\n",
        "# print(output_text)\n",
        "start_index = output_text.find(\"Image Tags: \")\n",
        "end_index = output_text.find(\"\\n\", start_index)\n",
        "\n",
        "# Extract and print the \"Image Tags\" line\n",
        "image_tags_line = output_text[start_index:end_index]\n",
        "print(image_tags_line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkeUFSsJhV_1",
        "outputId": "600c5a2d-ef40-4a8e-a44c-2176cba4880e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Tags:  armchair | blanket | lamp | carpet | couch | dog | floor | furniture | gray | green | living room | picture frame | pillow | plant | room | sit | stool | wood floor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('You selected', model)\n",
        "print('You selected', task)\n",
        "\n",
        "def run_inference(model, task):\n",
        "    if model == \"Tag2Text\" and task == \"one image\":\n",
        "        !python inference_tag2text.py  --image {image_path} \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth\n",
        "    elif model == \"Tag2Text\" and task == \"multiple images\":\n",
        "        x = !python batch_inference.py --image-dir {images_dir} \\\n",
        "        --pretrained pretrained/tag2text_swin_14m.pth --model-type tag2text\n",
        "        print(x)\n",
        "    elif model == \"RAM\" and task == \"one image\":\n",
        "        !python inference_ram.py  --image {image_path} \\\n",
        "        --pretrained pretrained/ram_swin_large_14m.pth\n",
        "    elif model == \"RAM\" and task == \"multiple images\":\n",
        "        # !python batch_inference.py --image-dir {images_dir} \\\n",
        "        # --pretrained pretrained/ram_swin_large_14m.pth --model-type ram\n",
        "\n",
        "        !python batch_inference.py \\\n",
        "        --model-type ram \\\n",
        "        --checkpoint pretrained/ram_swin_large_14m.pth \\\n",
        "        --dataset openimages_common_214\n",
        "    else:\n",
        "        print('Invalid model or task')\n",
        "\n",
        "\n",
        "run_inference(model, task)"
      ],
      "metadata": {
        "id": "xp95DJU5eFSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}